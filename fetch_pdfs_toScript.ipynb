{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=argparse.ArgumentParser()\n",
    "parser._optionals.title = \"Flag Arguments\"\n",
    "parser.add_argument('-pmids',help=\"Comma separated list of pmids to fetch. Must include -pmids or -pmf.\", default='%#$')\n",
    "parser.add_argument('-pmf',help=\"File with pmids to fetch inside, one pmid per line. Optionally, the file can be a tsv with a second column of names to save each pmid's article with (without '.pdf' at the end). Must include -pmids or -pmf\", default='%#$')\n",
    "parser.add_argument('-out',help=\"Output directory for fetched articles.  Default: fetched_pdfs\", default=\"fetched_pdfs\")\n",
    "parser.add_argument('-maxRetries',help=\"Change max number of retries per article on an error 104.  Default: 3\", default=3,type=int)\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #debugging\n",
    "# args={'pmids':'28388874',\n",
    "#       'pmf':'%#$',\n",
    "#       'out':'fetched_pdfs',\n",
    "#       'maxRetries':3,\n",
    "#       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv)==1:\n",
    "    parser.print_help(sys.stderr)\n",
    "    exit(1)\n",
    "if args['pmids']=='%#$' and args['pmf']=='%#$':\n",
    "    print \"Error: Either -pmids or -pmf must be used.  Exiting.\"\n",
    "    exit(1)\n",
    "if args['pmids']!='%#$' and args['pmf']!='%#$':\n",
    "    print \"Error: -pmids and -pmf cannot be used together.  Ignoring -pmf argument\"\n",
    "    args['pmf']='%#$'\n",
    "if not os.path.exists(args['out']):\n",
    "    print \"Output directory of {0} did not exist.  Created the directory.\".format(args['out'])\n",
    "    os.mkdir(args['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainUrl(url):\n",
    "    return \"/\".join(url.split(\"/\")[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePdfFromUrl(pdfUrl,directory,name):\n",
    "    t=requests.get(pdfUrl)\n",
    "    with open('{0}/{1}.pdf'.format(directory,name), 'wb') as f:\n",
    "        f.write(t.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(pmid,finders,name):\n",
    "  \n",
    "    uri = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id={0}&retmode=ref&cmd=prlinks\".format(pmid)\n",
    "    success = False\n",
    "    dontTry=False\n",
    "    if os.path.exists(\"{0}/{1}.pdf\".format(args['out'],pmid)): # bypass finders if pdf reprint already stored locally\n",
    "        print \"** Reprint #{0} already downloaded and in folder; skipping.\".format(pmid)\n",
    "        return\n",
    "    else:\n",
    "        #first, download the html from the page that is on the otherside of the pubmed API\n",
    "        req=requests.get(uri)\n",
    "        if 'pubmed' in req.url:\n",
    "            print \" ** Reprint {0} cannot be fetched as pubmed does not have a link to its pdf.\".format(pmid)\n",
    "            dontTry=True\n",
    "            success=True\n",
    "        soup=BeautifulSoup(req.content,'lxml')\n",
    "        \n",
    "        # loop through all finders until it finds one that return the pdf reprint\n",
    "        if not dontTry:\n",
    "            for finder in finders:\n",
    "                print \"Trying {0}\".format(finder)\n",
    "                pdfUrl = eval(finder)(req,soup)\n",
    "                if type(pdfUrl)!=type(None):\n",
    "                    savePdfFromUrl(pdfUrl,args['out'],name)\n",
    "                    success = True\n",
    "                    print \"** fetching of reprint {0} succeeded\".format(pmid)\n",
    "                    break\n",
    "       \n",
    "        if not success:\n",
    "            print \"** Reprint {0} could not be fetched with the current finders.\".format(pmid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genericCitationLabelled(req,soup): #if anyone has CSH access, I can check this.  Also, a PMID on CSH would help debugging\n",
    "    \n",
    "    possibleLinks=soup.find_all('meta',attrs={'name':'citation_pdf_url'})\n",
    "    if len(possibleLinks)>0:\n",
    "        print \"** fetching reprint using the 'generic citation labelled' finder...\"\n",
    "        pdfUrl=possibleLinks[0].get('content')\n",
    "        return pdfUrl\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_pdf_link(req,soup): #if anyone has a PMID that direct links, I can debug this better\n",
    "    \n",
    "    if req.content[-4:]=='.pdf':\n",
    "        print \"** fetching reprint using the 'direct pdf link' finder...\"\n",
    "        pdfUrl=req.content\n",
    "        return pdfUrl\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def science_direct(req,soup):\n",
    "    newUri=urllib.unquote(soup.find_all('input')[0].get('value'))\n",
    "    req=requests.get(newUri,allow_redirects=True)\n",
    "    soup=BeautifulSoup(req.content)\n",
    "\n",
    "    possibleLinks=soup.find_all('meta',attrs={'name':'citation_pdf_url'})\n",
    "    \n",
    "    if len(possibleLinks)>0:\n",
    "        print \"** fetching reprint using the 'science_direct' finder...\"\n",
    "        pdfUrl=possibleLinks[0].get('content')\n",
    "        return pdfUrl\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_central(req,soup):\n",
    "\n",
    "    possibleLinks=soup.find_all('a',re.compile('pdf'))\n",
    "    \n",
    "    possibleLinks=[x for x in possibleLinks if 'epdf' not in x.get('title').lower()] #this allows the pubmed_central finder to also work for wiley\n",
    "    \n",
    "    if len(possibleLinks)>0:\n",
    "        print \"** fetching reprint using the 'pubmed central' finder...\"\n",
    "        pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "        return pdfUrl\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acsPublications(req,soup):\n",
    "    possibleLinks=[x for x in soup.find_all('a') if type(x.get('title'))==str and ('high-res pdf' in x.get('title').lower() or 'low-res pdf' in x.get('title').lower())]\n",
    "    \n",
    "    if len(possibleLinks)>0:\n",
    "        print \"** fetching reprint using the 'acsPublications' finder...\"\n",
    "        pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "        return pdfUrl\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeneric(req,soup): # this finder has been renamed 'zeneric' instead of 'generic' to have it called last (as last resort)\n",
    "#     page = m.click p.links_with(:text  => /pdf|full[\\s-]?text|reprint/i, :href => /.pdf$/i)[0]\n",
    "    #page = m.click p.links_with(:text  => /pdf|full[\\s-]?text|reprint/i).and.href(/.pdf$/i)\n",
    "#     if len(possibleLinks)>0:\n",
    "#         print \"** fetching reprint using the 'pubmed central' finder...\"\n",
    "#         pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "#         return pdfUrl\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zframe(req,soup): # this finder has been renamed 'zframe' instead of 'frame' to have it called last (as last resort)\n",
    "#   page = m.click p.frame_with(:src => /.pdf/i)\n",
    "#     if len(possibleLinks)>0:\n",
    "#         print \"** fetching reprint using the 'pubmed central' finder...\"\n",
    "#         pdfUrl=getMainUrl(req.url)+possibleLinks[0].get('href')\n",
    "#         return pdfUrl\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "finders=[\n",
    "         'genericCitationLabelled',\n",
    "         'pubmed_central',\n",
    "         'science_direct',\n",
    "         'acsPublications',\n",
    "#          'zeneric', #removed until someone on github reports needing it, and then I will adapt it to python\n",
    "#          'zframe', #as above \n",
    "         'direct_pdf_link',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch pmid 28388874\n",
      "Trying genericCitationLabelled\n",
      "** fetching reprint using the 'generic citation labelled' finder...\n",
      "** fetching of reprint 28388874 succeeded\n"
     ]
    }
   ],
   "source": [
    "if args['pmids']!='%#$':\n",
    "    pmids=args['pmids'].split(\",\")\n",
    "    names=pmids\n",
    "else:\n",
    "    pmids=[line.strip().split() for line in open(args['pmf'])]\n",
    "    if len(pmids[0])==1:\n",
    "        pmids=[x[0] for x in pmids]\n",
    "        names=pmids.copy()\n",
    "    else:\n",
    "        names=[x[1] for x in pmids]\n",
    "        pmids=[x[0] for x in pmids]\n",
    "\n",
    "for pmid,name in zip(pmids,names):\n",
    "    print (\"Trying to fetch pmid {0}\".format(pmid))\n",
    "    retriesSoFar=0\n",
    "    while retriesSoFar<args['maxRetries']:\n",
    "        try:\n",
    "            fetch(pmid,finders,name)\n",
    "            retriesSoFar=args['maxRetries']\n",
    "        except Exception as e:\n",
    "            if type(e)==requests.ConnectionError and '104' in e[0][1][0]:\n",
    "                retriesSoFar+=1\n",
    "                if retriesSoFar<args['maxRetries']:\n",
    "                    print \"** fetching of reprint {0} failed from error {1}, retrying\".format(pmid,e)\n",
    "                else:\n",
    "                    print \"** fetching of reprint {0} failed from error {1}\".format(pmid,e)\n",
    "            else:\n",
    "                print \"** fetching of reprint {0} failed from error {1}\".format(pmid,e)\n",
    "                retriesSoFar=args['maxRetries']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "py27kerneldbfix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
